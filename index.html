---
layout: default
title: ICML Workshop on Stein's Method
---

		<div class="blurb">
			<h1>Workshop on Stein's Method in Machine Learning and Statistics <img src="beach.jpg" alt="beach", width="460", height="280", align="right", style="padding:30px;"></h1>
			<h4>International Conference on Machine Learning 2019</h4>
			<p><strong>Date:</strong> Saturday 15th June 2019.</p> 
			<p><strong>Location:</strong> Long Beach Convention Center - Room 104A.</p>
			
		</div> 

<div class="blurb">

	<h4> Outline </h4>
	<p>Stein's method is a technique from probability theory for bounding the distance between probability measures using differential and difference operators. Although the method was initially designed as a technique for proving central limit theorems, it has recently caught the attention of the machine learning (ML) community and has been used for a variety of practical tasks. Recent applications include goodness-of-fit testing, generative modeling, global non-convex optimisation, variational inference, de novo sampling, constructing powerful control variates for Monte Carlo variance reduction, and measuring the quality of Markov chain Monte Carlo algorithms.</p> 

	<p>Although Stein's method has already had significant impact in ML, most of the applications only scratch the surface of this rich area of research in probability theory. Significant gains could be made by encouraging both communities to interact directly, and this workshop aims to facilitate this discussion.</p>
 
	<p>The workshop will begin with an introduction to Stein's method which will be accessible for researchers in machine learning who are unfamiliar with the topic. It will then consist of an alternating sequence of invited talks from machine learning researchers and experts in Stein's method, which will highlight both foundational topics and applications in machine learning and statistics. The workshop will also include a session for contributed posters and will conclude with a panel discussion to elicit a concise summary of the state of the field. </p>

	<p><strong> Registration: </strong> Registration for this workshop is through the main <strong><a href="https://icml.cc/"> ICML website</a></strong>. We encourage participants to register as soon as possible as places are limited and often fill up quickly.</p>
</div>


<div class="blurb">

	<h4> Timetable </h4>

	<ul>
		<li>8:30-8:45: Overview of the day </li>
		<li>8:45-9:45: Tutorial - Larry Goldstein <a href="https://steinworkshop.github.io/slides/larry_goldstein.pdf"><strong>[Slides]</strong></a></li>
		<li>9:45-10:30: Invited Talk - Anima Anandkumar <a href="https://steinworkshop.github.io/slides/anima_anandkumar.pptx"><strong>[Slides]</strong></a></li>
		<li>10:30-11:00: Break</li>
		<li>11:00-11:45: Invited Talk - Arthur Gretton <a href="https://steinworkshop.github.io/slides/arthur_gretton.pdf"><strong>[Slides]</strong></a></li>
		<li>11:45-12:07: Invited Talk - Wilson Chen <a href="https://steinworkshop.github.io/slides/wilson_chen.pdf"><strong>[Slides]</strong></a><br/></li>
		<li>12:07-12:30: Invited Talk - Francois-Xavier Briol <a href="https://steinworkshop.github.io/slides/francoisxavier_briol.pdf"><strong>[Slides]</strong></a></li>
		<li>12:30-13:45: Lunch and Poster Session</li>
		<li>13:45-14:30: Invited Talk - Yingzhen Li <a href="https://steinworkshop.github.io/slides/yingzhen_li.pdf"><strong>[Slides]</strong></a></li>
		<li>14:30-15:15: Invited Talk - Ruiyi Zhang <a href="https://steinworkshop.github.io/slides/ruiyi_zhang.pdf"><strong>[Slides]</strong></a></li>
		<li>15:15-15:45: Break</li>
		<li>15:45-16:30: Invited Talk - Paul Valiant <a href="https://steinworkshop.github.io/slides/paul_valiant"><strong>[Slides]</strong></a> </li>
		<li>16:30-17:15: Invited Talk - Louis Chen <a href="https://steinworkshop.github.io/slides/louis_chen.pdf"><strong>[Slides]</strong></a></li>
		<li>17:15-18:00: Panel Discussion - All speakers</li>
	</ul> 
	
</div>



<div class="blurb">

	<h4> Invited Speakers </h4>

	<ul>

		<li> <strong><a href="http://tensorlab.cms.caltech.edu/users/anima/"> Anima Anandkumar (California Institute of Technology, US).</a></strong> <br/> <br/>

			
			<strong> Title: </strong> Stein’s method for understanding optimization in neural networks. <br/>

			<strong> Abstract: </strong> Training neural networks is a challenging non-convex optimization problem. Stein’s method provides a novel way to change optimization problem to a tensor decomposition problem for guaranteed training of two-layer neural networks. We provide risk bounds for our proposed method, with a polynomial sample complexity in the relevant parameters, such as input dimension and number of neurons. Our training method is based on tensor decomposition, which provably converges to the global optimum, under a set of mild non-degeneracy conditions. This provides insights into role of generative process for tractability of supervised learning. <br/>

			<strong> Bio: </strong> Anima is Bren Professor in the Department of Computing and Mathematical Sciences at the California Institute of Technology and Director of Machine Learning Research at NVIDIA. She has several papers establishing connections between Stein's method and discriminative learning and tensor methods. <br/>


			<a href="https://steinworkshop.github.io/slides/anima_anandkumar.pptx"><strong>[Slides]</strong></a><br/>

		</li>

		<li> <strong><a href="https://fxbriol.github.io/"> Francois-Xavier Briol (University of Cambridge, UK).</a></strong> <br/><br/>

			
			<strong> Title: </strong> Minimum Stein discrepancy estimators. <br/>

			<strong> Abstract: </strong> When maximum likelihood estimation is infeasible, one often turns to score matching, contrastive divergence, or minimum probability flow learning to obtain tractable parameter estimates. In this talk, I will present a unifying perspective of these techniques as minimum Stein discrepancy estimators and use this lens to design new diffusion kernel Stein discrepancy (DKSD) and diffusion score matching (DSM) estimators with complementary strengths.  Results establishing consistency, asymptotic normality, and robustness of DKSD and DSM estimators will be presented, along with practical details of how inference can be performed in practice using Riemannian gradient descent algorithms. Advantages over other methods will be demonstrated for specific classes of models, specifically those with non-smooth densities or with heavy tailed distributions. <br/>

			<strong> Bio: </strong> Francois-Xavier is a research associate in the Department of Engineering at the University of Cambridge and a visiting researcher at the Alan Turing Institute within the Data-Centric Engineering programme. His research focuses on inference and computation for probabilistic models, and has worked on applications of Stein's method to Bayesian computation. <br/>

			<a href="https://steinworkshop.github.io/slides/francoisxavier_briol.pdf"><strong>[Slides]</strong></a><br/>


		</li>

		<li> <strong><a href="http://www.math.nus.edu.sg/~lhychen/"> Louis Chen (National University Singapore, Singapore). </a></strong> <br/><br/>

			
			<strong> Title: </strong> Palm theory, random measures and Stein couplings. <br/>

			<strong> Abstract: </strong> In this talk, we present a general Kolmogorov bound for normal approximation, proved using Stein’s method. We will apply this bound to random measures using Palm theory and coupling, with applications to stochastic geometry. We will also apply this bound to Stein couplings, which include local dependence, exchangeable pairs, size-bias couplings and more as special cases. This talk is based on joint work with Adrian Roellin and Aihua Xia. <br/>

			<strong> Bio: </strong> Louis is Emeritus Professor in the Department of Mathematics at the National University of Singapore. On top of his dozens of papers on Stein's method, he has co-authored a book and edited two others on the topic. <br/>

			<a href="https://steinworkshop.github.io/slides/louis_chen.pdf"><strong>[Slides]</strong></a><br/>
		</li>

		

		<li> <strong><a href="http://www.gatsby.ucl.ac.uk/~gretton/"> Arthur Gretton (University College London, UK).</a></strong> <br/><br/>
			<strong> Title: </strong> Relative goodness-of-fit tests for models with latent variables. <br/> 

			<strong> Abstract: </strong> I will describe a nonparametric, kernel-based test to assess the relative goodness of fit of latent variable models with intractable unnormalized densities. The test generalises the kernel Stein discrepancy (KSD) tests of (Liu et al., 2016, Chwialkowski et al., 2016, Yang et al., 2018, Jitkrittum et al., 2018) which require exact access to unnormalized densities. We will rely on the simple idea of using an approximate observed-variable marginal in place of the exact, intractable one. As the main theoretical contribution, the new test has a well-controlled type-I error, once we have properly corrected the threshold. In the case of models with low-dimensional latent structure and high-dimensional observations, our test significantly outperforms the relative maximum mean discrepancy test, which cannot exploit the latent structure. <br/>

			<strong> Bio: </strong> Arthur is Professor at the Gatsby Computational Neuroscience Unit at University College London. He is the author of several papers at the intersection of kernel methods and Stein's method for hypothesis testing, and received a NeurIPS 2017 Best Paper award for a novel approach to goodness-of-fit testing using Stein's method. <br/> 

			<a href="https://steinworkshop.github.io/slides/arthur_gretton.pdf"><strong>[Slides]</strong></a><br/>

		</li>

		<li> <strong><a href="http://yingzhenli.net/home/en/"> Yingzhen Li (Microsoft Research Cambridge, UK).</a></strong> <br/> <br/>

			
			<strong> Title: </strong> Gradient estimation for implicit models with Stein's method. <br/> 

			<strong> Abstract: </strong> Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. Gradient based optimization/sampling methods are often applied to train these models, however without tractable densities, the objective functions often need to be approximated. In this talk I will motivate gradient estimation as another approximation approach for training implicit models and perform Monte Carlo based approximate inference. Based on this view, I will then present the Stein gradient estimator which estimates the score function of an implicit model density. I will discuss connections of this approach to score matching, kernel methods, denoising auto-encoders, etc., and show application cases including entropy regularization for GANs, and meta-learning for stochastic gradient MCMC algorithms.<br/> 

			<strong> Bio: </strong> Yingzhen is a researcher at Microsoft Research Cambridge, where she works on Bayesian deep learning including graphical models, fast inference methods and uncertainty estimates for downstream tasks. She is particularly interested in connections between Stein's method and gradient estimators for implicit models. <br/> 

			<a href="https://steinworkshop.github.io/slides/yingzhen_li.pdf"><strong>[Slides]</strong></a><br/>

		</li>

		<li> <strong><a href="http://cs.brown.edu/~pvaliant/"> Paul Valiant (Brown University, US).</a></strong> <br/> <br/>

			<strong> Title: </strong> How the Ornstein-Uhlenbeck process drives generalization for deep learning. <br/> 

			<strong> Abstract: </strong> After discussing the Ornstein-Uhlenbeck process and how it arises in the context of Stein's method, we turn to an analysis of the stochastic gradient descent method that drives deep learning. We show that certain noise that often arises in the training process induces an Ornstein-Uhlenbeck process on the learned parameters. This process is responsible for a weak regularization effect on the training that, once it reaches stable points, will have provable found the "simplest possible" hypothesis consistent with the training data. At a higher level, we argue how some of the big mysteries of the success of deep learning may be revealed by analyses of the subtle stochastic processes which govern deep learning training, a natural focus for this community.  <br/> 

			<strong> Bio: </strong> Paul is assistant professor in the department of Computer Science at Brown. He is particularly interested in Stein's method for applications in machine learning, and in particular for understanding the behaviour of deep learning algorithms.<br/> 

			<a href="https://steinworkshop.github.io/slides/paul_valiant.pptx"><strong>[Slides]</strong></a><br/>

		</li>

		<li> <strong><a href="https://users.cs.duke.edu/~ryzhang/"> Ruiyi Zhang (Duke University, US).</a></strong> <br/> <br/>

			<strong> Title: </strong> On Wasserstein Gradient Flows and Particle-Based Variational Inference <br/> 

			<strong> Abstract: </strong> Particle-based variational inference methods (ParVIs), such as models associated with Stein variational gradient descent, have gained attention in the Bayesian inference literature, for their capacity to yield flexible and accurate approximations. We explore ParVIs from the perspective of Wasserstein gradient flows, and make both theoretical and practical contributions. We unify various finite-particle approximations that existing ParVIs use, and recognize that the approximation is essentially a compulsory smoothing treatment, in either of two equivalent forms. This novel understanding reveals the assumptions and relations of existing ParVIs, and also inspires new ParVIs. We propose an acceleration framework and a principled bandwidth-selection method for general ParVIs; these are based on the developed theory and leverage the geometry of the Wasserstein space. Experimental results show the improved convergence by the acceleration framework and enhanced sample accuracy by the bandwidth-selection method. <br/> 

			<strong> Bio: </strong>  Ruiyi is a PhD student in the department of computer science at Duke University. Ruiyi has contributed methodology on particle-based variational inference, and is particular interested in connections with Stein Variational Gradient Descent. <br/> 

			<a href="https://steinworkshop.github.io/slides/ruiyi_zhang.pdf"><strong>[Slides]</strong></a><br/>

		</li>
		

	</ul>

</div>
	

<div class="blurb">


	<h4> List of Accepted Posters </h4>

	<ol>

	<li> Stein's Method for Error Analysis of Multivariate Density Estimation using Models with Normalizing Flow. </li>

	<li> Stein methods for Robot Navigation. </li> 

	<li> Estimation and Sampling of Unnormalized Statistical Models with Stein Score Matching. </li>  

	<li> Stein’s Method for Policy Gradients Methods in Deep Reinforcement Learning.</li>

	<li> Spectral Estimators for Gradient Fields of Log-Densities.</li>

	<li> Relative Kernel Stein Discrepancy for Multiple Model Comparison.</li>

	<li> Stein Point Markov Chain Monte Carlo.</li>

	<li> Stein’s Lemma for the Reparameterization Trick with Gaussian Mixtures.</li>

	<li> A Stein-Papangelou Goodness-of-Fit Test for Point Processes.</li>

	<li> Adaptive MCMC via combining local samplers.</li>

	<li> To choose or not to choose the Prior. That’s the question!</li>

	<li> Stein Variational Online Changepoint Detection with Applications to Hawkes Processes and Neural Networks. </li>

	<li> Active Domain Randomization. </li>

	<li> Sobolev Descent. </li>

	<li> Yet Another Look at SVGD. </li>

	<li> Multi-Agent Learning Using Malliavin-Stein Variational Gradient Descent. </li>

	<li> Stein's method for computing inverse operators. </li> 

	<li> Using Stein's method to find bounds for the multivariate normal approximation of the group sequential maximum likelihood estimator </li>

	</ol>

</div>


<div>

	<h4>Organisation</h4>

	<ul>
			<li> <strong><a href="https://fxbriol.github.io/"> Francois-Xavier Briol (University of Cambridge, UK)</a></strong>. Francois-Xavier is a research associate in the Department of Engineering at the University of Cambridge and a visiting researcher at the Alan Turing Institute within the Data-Centric Engineering programme. His research focuses on inference and computation for probabilistic models, and has worked on applications of Stein's method to Bayesian computation. </li>

			<li> <strong><a href="https://web.stanford.edu/~lmackey/"> Lester Mackey (Microsoft Research, US)</a></strong>. Lester is a Researcher at Microsoft Research New England and an Adjunct Professor of Statistics at Stanford University. He has been actively developing Stein's method tools for a variety of problems in machine learning and probabilistic inference including global non-convex optimization, de novo sampling, hypothesis testing, causal inference, and Markov chain Monte Carlo parameter tuning and sampler selection. </li>

			<li> <strong><a href="http://oates.work/"> Chris J. Oates (Newcastle University, UK)</a></strong>. Chris is Chair in Statistics at Newcastle University, UK, and group leader for the Data-Centric Engineering programme at the Alan Turing Institute, UK. His research interests include using Stein's method for posterior computation in the Bayesian statistical context.</li>

			<li> <strong><a href="https://www.cs.utexas.edu/~lqiang/"> Qiang Liu (UT Austin, US)</a></strong>. Qiang is an assistant professor of computer science at UT Austin. His research focuses on probabilistic learning and approximate inference, including application of Stein's method to addressing algorithmic challenges in approximate inference and learning.</li>

			<li> <strong><a href="https://dornsife.usc.edu/larry-goldstein/"> Larry Goldstein (University of Southern California, US)</a></strong>. Larry is Professor of Mathematics at the University of Southern California. His research interests center on Stein's method for distributional approximation and its applications in high dimensional statistical contexts. He is co-author of the 2010 Springer text on 'Normal approximation by Stein's method'.</li>
	</ul>


</div>
